<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-2VYEP6CXDE"></script><script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag("js", new Date());  gtag("config", "G-2VYEP6CXDE");</script><link rel="stylesheet" href="../init.css"><title>Will Nvidia’s huge bet on artificial-intelligence chips pay off?</title></head><body><div class="ds-layout-grid ds-layout-grid--edged layout-article-header"><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://www.economist.com/business/","name":"Business"}},{"@type":"ListItem","position":2,"item":{"@id":"https://www.economist.com/printedition/2021-08-07","name":"Aug 7th 2021 edition"}}]}
</script><div class="article__section"><strong itemprop="articleSection"><span class="article__section-headline"><a href="/business/">Business</a></span></strong><a class="article__section-edition" data-test-id="Article Datetime" href="/printedition/2021-08-07"><span>Aug 7th 2021<!-- --> edition</span></a></div><header class="article__header"><h1><span class="article__subheadline" data-test-id="Article Subheadline">Veni, Nvidia, vici</span><br/><span class="article__headline" data-test-id="Article Headline" itemprop="headline">Will Nvidia’s huge bet on artificial-intelligence chips pay off?</span></h1><h2 class="article__description" data-test-id="Article Description" itemprop="description">The unassuming chipmaking giant was early to the AI revolution—and remains ahead of rivals</h2></header><div class="article__lead-image" data-test-id="Lead Image"><div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject"><meta content="https://www.economist.com/img/b/1280/720/90/sites/default/files/images/2021/08/articles/main/20210807_wbd011.jpg" itemprop="url"/><img alt="" height="720" sizes="(min-width: 1440px) 940px, (min-width: 1080px) 75vw, (min-width: 960px) 90vw, (min-width: 800px) 720px, 95vw" src="../image/20210807_wbd011.jpg" srcset="../image/20210807_wbd011.jpg" width="1280"/></div></div><hr class="ds-rule layout-article-header__rule"/><div class="advert right hidden advert--right-rail"><div><div class="adcontainer" id="econright-r0"></div></div></div></div><div class="ds-layout-grid ds-layout-grid--edged layout-article-body"><div class="layout-sticky-rail"><div class="layout-sticky-rail-advert-wrapper"><div class="advert right hidden advert--right-rail advert--sticky-rail"><div><div class="adcontainer" id="econright-r1"></div></div></div></div></div><p class="article__body-text article__body-text--dropcap" data-caps="initial"><span data-caps="initial">“W</span><small>E’RE ALWAYS</small> 30 days away from going out of business,” is a mantra of Jen-Hsun Huang, co-founder of Nvidia. That may be a little hyperbolic coming from the boss of a company whose market value has increased from $31bn to $505bn in five years and which has eclipsed Intel, once the world’s mightiest chipmaker, by selling high-performance semiconductors for gaming and artificial intelligence (<small>AI</small>). But only a little. As Mr Huang observes, Nvidia is surrounded by “giant companies pursuing the same giant opportunity”. To borrow a phrase from Intel’s co-founder, Andy Grove, in this fast-moving market “only the paranoid survive”.</p><div class="article-audio-player"><figure class="article-audio-player__figure"><figcaption>Listen to this story</figcaption><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="../audio/will-nvidias-huge-bet-on-artificial-intelligence-chips-pay-off.mp3" title="Will Nvidia’s huge bet on artificial-intelligence chips pay off?"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure><p class="article-audio-player__cta">Enjoy more audio and podcasts on<!-- --> <a href="https://apps.apple.com/app/apple-store/id1239397626?pt=344884&amp;ct=article%20audio%20player&amp;mt=8" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://play.google.com/store/apps/details?id=com.economist.lamarr&amp;referrer=utm_source%3Darticle%2520audio%2520player" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</p></div><div class="article__body-text-image"><figure><div data-slim="1" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject"><meta content="https://www.economist.com/img/b/608/662/90/sites/default/files/images/2021/08/articles/body/20210807_wbc115.png" itemprop="url"/><img alt="" loading="lazy" sizes="300px" src="../image/20210807_wbc115.png" srcset="../image/20210807_wbc115.png"/></div></figure><p class="article__body-text">Constant vigilance has served Nvidia well. Between 2016 and 2021 revenues grew by 233%. Operating profit more than doubled in the past five years, to $4.5bn (see chart 1). In the three months to May sales shot up by 84%, year on year; gross margin reached 64%. Although Intel’s revenues are four times as large and it fabricates chips as well as designing them, investors value Nvidia’s design-only business more highly (twice as much in terms of market capitalisation). The data centres that make up the computing clouds of Amazon, Google, Microsoft and China’s Alibaba all use its products. So do all big information-technology (<small>IT</small>) firms, as well as countless scientific-research teams in fields from drug discovery to climate modelling. It has created a broad, deep “moat” that protects its competitive advantage.</p></div><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-1"></div></div></div><p class="article__body-text">Now Mr Huang wants to make the moat broader and deeper still. In September Nvidia said it would buy Arm, a Britain-based firm that designs zippy, energy-efficient chips used in most smartphones, for $40bn. The idea is to use Arm’s design prowess to engineer central processing units (<small>CPU</small>s) for data centres and <small>AI</small> uses that would complement Nvidia’s existing strength in specialised chips known as graphics-processing units (<small>GPU</small>s). Regulators in America, Britain, China and the <small>EU</small> must all approve the deal. If they do—a considerable “if”, given both firms’ market power in their respective domains—Nvidia’s position in one of computing’s hottest fields would look near-unassailable.</p><p class="article__body-text"><html><body><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="../audio/blockofftheoldchips-nvidia-sfraughtmerger.mp3" title=""><p></p></audio></body></html></p><p class="article__body-text">Mr Huang, whose family immigrated to America from Taiwan when he was a child, founded Nvidia in 1993. For its first 20 years or so the firm made <small>GPU</small>s that helped video games look lifelike. In the past decade, though, it turned out that <small>GPU</small>s also excel in another futuristic, but less frivolous, area of computing: they dramatically speed up how fast machine-learning algorithms can be trained to perform tasks by feeding them oodles of data. Four years ago Mr Huang startled Wall Street with a blunt assessment of Nvidia’s prospects in what has become known as accelerated computing. It could “work out great”, he said, “or terribly”. Regardless, the firm was “all in”.</p><div class="article__body-text-image"><figure><div data-slim="1" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject"><meta content="https://www.economist.com/img/b/608/662/90/sites/default/files/images/2021/08/articles/body/20210807_wbc117.png" itemprop="url"/><img alt="" loading="lazy" sizes="300px" src="../image/20210807_wbc117.png" srcset="../image/20210807_wbc117.png"/></div></figure><p class="article__body-text">Around half of Nvidia’s revenues of $17bn a year still come from gaming chips (see chart 2). These are also adept at solving the mathematical puzzles that underpin ethereum, a popular cryptocurrency. This has at times injected crypto-like volatility into <small>GPU</small> sales, which partly caused a near-50% fall in Nvidia’s share price in 2018. Another slug of sales comes from selling chips that speed up features other than graphics or <small>AI</small> to hardware-makers.</p><p class="article__body-text">But the <small>AI</small> business is growing fast. It includes specialised chips and software that lets programmers fine-tune them—itself made possible by Mr Huang’s earlier bet, which some investors criticised at the time as a costly distraction. In 2004 he started investing in “Cuda”, a base software layer that enables such fine-tuning, and implanting it in all Nvidia chips.</p><p class="article__body-text">A lot of these systems end up in servers, the powerful computers behind data centres’ processing oomph. Sales to data centres contribute 36% of total revenues, up from 25% in early 2019 and nearly as much as gaming <small>GPU</small>s. As companies across industries adopt <small>AI</small>, the share of Nvidia’s data-centre sales going to the big cloud providers has declined from 100% to half.</p><p class="article__body-text">Today its <small>AI</small> hardware-software combo is designed to work seamlessly with the machine-learning algorithms collected in libraries such as TensorFlow, kept by Google, and Facebook’s PyTorch. The firm has created programs to hook its hardware and software up to the <small>IT</small> systems of big business clients with <small>AI</small> projects of their own. This makes the job of <small>AI</small> developers immeasurably easier, says a former Nvidia executive. Nvidia is also getting into “inference”: running <small>AI</small> models, hitherto the preserve of <small>CPU</small>s, not merely training them. Huge, real-time models like those used for speech recognition or content recommendation increasingly need specialised <small>GPU</small>s to perform well, says Ian Buck, head of Nvidia’s accelerated-computing business.</p></div><h2>Armed and dangerous</h2><p class="article__body-text">This is where Arm comes in. Owning it would give Nvidia the <small>CPU</small> chops to complement those in <small>GPU</small>s, as well as its new abilities in network-interface cards needed in server farms (in 2019 it bought Mellanox, a specialist in the field). In April Nvidia unveiled plans for its first data-centre <small>CPU</small>, Grace, a high-end chip based on an Arm design. Arm’s energy-efficient chips may go into <small>AI</small> wares for “edge computing”—in self-driving cars, factory robots and other uses far from data centres, where power-hungry <small>GPU</small>s may not be ideal.</p><p class="article__body-text">Transistors in microprocessors are already the size of a few atoms, so have little room to shrink. Tricks such as outsourcing computing to the cloud or using software to split a physical computer into several virtual machines may run their course. So businesses are expected to turn to accelerated computing as a way to gain processing power without splurging on ever more <small>CPU</small>s. Over the next 5-10 years, as <small>AI</small> becomes more common, up to half of the $80bn-90bn that is spent annually on servers could shift to Nvidia’s accelerated-computing model, reckons Stacy Rasgon of Bernstein, a broker. Of that, half could go on accelerated chips, a market which Nvidia’s <small>GPU</small>s dominate, he says. Nvidia thinks the global market for accelerated computing, including data centres and the edge, will be more than $100bn a year.</p><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-2"></div></div></div><p class="article__body-text">Nvidia is not the only one to spy an opportunity. Competitors are proliferating, from startups to other chipmakers and the tech giants. Firms such as Tenstorrent, Untether <small>AI</small>, Cerebras and Groq are all trying to make chips even better suited to <small>AI</small> than Nvidia’s <small>GPU</small>s, which for all their virtues can use lots of power and be fiddly to program. Graphcore, a British firm, is promoting its “intelligence-processing unit”.</p><p class="article__body-text">__________</p><ul><li><em>For more expert analysis of the biggest stories in economics, business and markets, <a href="./.html">sign up to Money Talks</a>, our weekly newsletter</em></li></ul><p class="article__body-text"><em>__________</em></p><p class="article__body-text">In 2019 Intel bought an Israeli <small>AI</small>-chip startup called Habana Labs. Amazon Web Services (<small>AWS</small>), the e-emporium’s cloud division, will soon start offering Habana’s Gaudi accelerators to its customers. It claims that the Gaudi chips, though slower than Nvidia’s <small>GPU</small>s, are nevertheless 40% cheaper relative to performance. Advanced Micro Devices (<small>AMD</small>), a veteran chipmaker that is Nvidia’s main rival in the gaming market and Intel’s in <small>CPU</small>s, is finalising a $35bn purchase of Xilinx, which makes another kind of accelerator chip called field programmable gate arrays (<small>FPGA</small>s).</p><p class="article__body-text">A bigger threat comes from Nvidia’s biggest clients. The cloud giants are all designing their own custom silicon. Google was the first, with its “tensor-processing unit”. Microsoft’s Azure cloud division opted for <small>FPGA</small>s. Baidu, China’s search giant, has “Kunlun” chips for <small>AI</small> and<!-- --> Alibaba, its e-commerce titan, has Hanguang 800. <small>AWS</small> already has a chip designed for inference, called Inferentia, and one coming for training. “The risk is that in ten years’ time <small>AWS</small> will offer a cheap <small>AI</small> box with all <small>AWS</small>-made components,” says the former Nvidia executive. Mark Lipacis at Jefferies, an investment bank, notes that since mid-2020 <small>AWS</small> has put Inferentia into an ever-greater share of its offering to customers, potentially at Nvidia’s expense.</p><p class="article__body-text">As for the Arm acquisition, it is far from a done deal. Arm’s customers include all the world’s chipmakers as well as <small>AWS</small> and Apple, which uses Arm chips in iPhones. Some have complained that Nvidia could restrict access to the chip designer’s blueprints. The Graviton2, <small>AWS</small>’s tailor-made server chip, is based on an Arm design. Nvidia says it has no plans to change Arm’s business model. Western regulators have yet to decide whether to approve the deal. Britain’s competition authority, which had until July 30th to scrutinise it, is expected to be among the first to issue a ruling. China is unlikely to welcome an American takeover of an important supplier to its own tech firms, which is currently owned by SoftBank, a Japanese tech group.</p><p class="article__body-text">Even if one of the antitrust watchdogs puts paid to the deal, however, Nvidia’s prospects look bright. Intel has overpromised many things, including accelerated computing, for years, and has mostly not delivered. Venture capitalists have become markedly less enthusiastic about backing startups that are taking on Nvidia and its software, and the tech giants investing in accelerated computing, observes Paul Teich of Equinix, an American data-centre operator. As for <small>AWS</small> and the rest of big tech, they have other things on their plates and lack Nvidia’s clear focus on accelerated computing. Nvidia says that, measured by actual use by businesses, it has not ceded market share to <small>AWS</small>’s Inferentia.</p><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-3"></div></div></div><p class="article__body-text">Mr Huang says that it is the expense of training and running <small>AI</small> applications that matters, not the cost of hardware components. And on that measure, he insists, “we are unrivalled on price-for-performance.” None of Nvidia’s rivals possesses its software ecosystem. And it has a proven ability to switch gears and capitalise on good luck. “They’re always looking around at what’s out there,” enthuses another former executive. And with an entrenched position, Mr Lipacis says, it also benefits from inertia.</p><p class="article__body-text">Investors have not forgotten the plunge in Nvidia’s share price in 2018. It may still be partly tied to the fortunes of the crypto market. Holding Nvidia stock requires a strong stomach, says Mr Rasgon of Bernstein. Nvidia may present itself as a pillar of the computing industry, but it remains an aggressive, founder-led firm that behaves like a startup. Sprinkle in some paranoia, and it will be hard to disrupt. <span data-ornament="ufinish">■</span></p><p class="article__body-text"><em>An early version of this article was published online on August 1st 2021</em></p><p class="article__body-text"><em><em>For more expert analysis of the biggest stories in economics, business and markets, <a data-analytics="in_body:link_1:para_10" data-saferedirecturl="https://www.google.com/url?q=https://www.economist.com/moneytalks/&amp;source=gmail&amp;ust=1628284234418000&amp;usg=AFQjCNGviR-zkpwefPKZAIJbeHx5M_SGCQ" href="./.html">sign up to Money Talks</a>, our weekly newsletter.</em></em></p><p class="article__footnote" data-test-id="Footnote">This article appeared in the Business section of the print edition under the headline "Veni, Nvidia, Vici"</p><div class="layout-article-links layout-article-promo"><a class="ds-actioned-link ds-actioned-link--reuse-this-content" href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Will%20Nvidia%E2%80%99s%20huge%20bet%20on%20artificial-intelligence%20chips%20pay%20off%3F&amp;publicationDate=2021-08-05&amp;contentID=%2Fcontent%2F0if4mq0jre8hqjaamu7h9j5mii4njp24&amp;type=A&amp;orderBeanReset=TRUE" target="_blank"><span>Reuse this content</span></a><a class="ds-actioned-link ds-actioned-link--the-trust-project" href="https://www.economist.com/frequently-asked-questions"><span>The Trust Project</span></a></div></div></body></html>