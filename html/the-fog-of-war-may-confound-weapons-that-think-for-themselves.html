<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><link rel="stylesheet" href="../init.css"><title>The fog of war may confound weapons that think for themselves</title></head><body><div class="ds-layout-grid ds-layout-grid--edged layout-article-header"><div class="article__lead-image" data-test-id="Lead Image"><div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject"><meta content="https://www.economist.com/img/b/1280/720/90/sites/default/files/images/2021/05/articles/main/20210522_stp512.jpg" itemprop="url"/><img alt="" height="720" sizes="(min-width: 1440px) 940px, (min-width: 1080px) 75vw, (min-width: 960px) 90vw, (min-width: 800px) 720px, 95vw" src="../image/20210522_stp512.jpg" srcset="../image/20210522_stp512.jpg" width="1280"/></div></div><header class="article__header"><h1><span class="article__subheadline" data-test-id="Article Subheadline">Autonomous weapons</span><br/><span class="article__headline" data-test-id="Article Headline" itemprop="headline">The fog of war may confound weapons that think for themselves</span></h1><p class="article__description" data-test-id="Article Description" itemprop="description">Some states want a ban. But would it be respected?</p></header><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://www.economist.com/science-and-technology/","name":"Science & technology"}},{"@type":"ListItem","position":2,"item":{"@id":"https://www.economist.com/printedition/2021-05-29","name":"May 29th 2021 edition"}}]}
</script><div class="article__section"><strong itemprop="articleSection"><span class="article__section-headline"><a href="/science-and-technology/">Science &amp; technology</a></span></strong><a class="article__section-edition" data-test-id="Article Datetime" href="/printedition/2021-05-29"><span>May 29th 2021<!-- --> edition</span></a></div><hr class="ds-rule layout-article-header__rule"/><div class="advert right hidden advert--right-rail" id=""><div><div id="econright-r0"></div></div></div></div><div class="ds-layout-grid ds-layout-grid--edged layout-article-body" itemprop="text"><div class="layout-sticky-rail"><div class="layout-sticky-rail-advert-wrapper"><div class="advert right hidden advert--right-rail advert--sticky-rail" id=""><div><div id="econright-r1"></div></div></div></div></div><p class="article__body-text article__body-text--dropcap" data-caps="initial"><span data-caps="initial">T</span><small>HE SKIES</small> of Israel have lit up in recent weeks with the sinuous trails of interceptors colliding with thick volleys of rockets fired from Gaza. These spectacular aerial duels show autonomous weapons at work. For, though each launcher of Iron Dome, Israel’s missile-defence system, is manned by soldiers, only a computer can keep up with the most intense barrages. “Sometimes in large salvos, you can't have men in the loop for every rocket,” says one Israeli soldier. It is largely an algorithm that decides where and when to fire.</p><div class="article-audio-player"><figure class="article-audio-player__figure"><figcaption>Listen to this story</figcaption><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/081%20Science%20and%20technology%20-%20Autonomous%20weapons-713153750474fa7840ab7ff2eb7e7dc0.mp3" title="The fog of war may confound weapons that think for themselves"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure><p class="article-audio-player__cta">Enjoy more audio and podcasts on<!-- --> <a href="https://apps.apple.com/app/apple-store/id1239397626?pt=344884&amp;ct=article%20audio%20player&amp;mt=8" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://play.google.com/store/apps/details?id=com.economist.lamarr&amp;referrer=utm_source%3Darticle%2520audio%2520player" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</p></div><p class="article__body-text">Iron Dome is a defensive system that attacks physical objects (the incoming rockets) in an aerial battle-theatre devoid of immediate civilian bystanders, albeit that falling debris could injure or kill someone. But one day similar latitude might be given to offensive weapons which fire at human enemies on the ground, in more crowded places. A new report by the United Nations Institute for Disarmament Research (<small>UNIDIR</small>), a think-tank, explains why that will be far harder than knocking rockets out of the sky.</p><p class="article__body-text"><a href="/briefing/2019/01/19/autonomous-weapons-and-the-new-laws-of-war">Autonomous systems</a> rely on artificial intelligence (<small>AI</small>), which in turn relies on data collected from those systems’ surroundings. When these data are good—plentiful, reliable and similar to the data on which the system’s algorithm was trained—<small>AI</small> can excel. But in many circumstances data are incomplete, ambiguous or overwhelming. Consider the difference between radiology, in which algorithms outperform human beings in analysing <small>X</small>-ray images, and self-driving cars, which still struggle to make sense of a cacophonous stream of disparate inputs from the outside world. On the battlefield, that problem is multiplied.</p><div class="advert incontent hidden advert--inline" id=""><div><div id="econ-1"></div></div></div><p class="article__body-text">“Conflict environments are harsh, dynamic and adversarial,” says <small>UNIDIR</small>. Dust, smoke and vibration can obscure or damage the cameras, radars and other sensors that capture data in the first place. Even a speck of dust on a sensor might, in a particular light, mislead an algorithm into classifying a civilian object as a military one, says Arthur Holland Michel, the report’s author. Moreover, enemies constantly attempt to fool those sensors through camouflage, concealment and trickery. Pedestrians have no reason to bamboozle self-driving cars, whereas soldiers work hard to blend into foliage. And a mixture of civilian and military objects—evident on the ground in Gaza in recent weeks—could produce a flood of confusing data.</p><p class="article__body-text">The biggest problem is that algorithms trained on limited data samples would encounter a much wider range of inputs in a war zone. In the same way that recognition software trained largely on white faces struggles to recognise black ones, an autonomous weapon fed with examples of Russian military uniforms will be less reliable against Chinese ones. “<small>AI</small> systems tend to be very brittle against anything that wasn’t covered in their development or testing,” says Mr Holland Michel. An autonomous system effective against one country might be unusable against a foe with superior camouflage, for instance.</p><p class="article__body-text">Despite these limitations, the technology is already trickling onto the battlefield. In its war with Armenia last year, <a href="/europe/2020/10/08/the-azerbaijan-armenia-conflict-hints-at-the-future-of-war">Azerbaijan unleashed</a> Israeli-made loitering munitions theoretically capable of choosing their own targets. Ziyan, a Chinese company, boasts that its Blowfish <small>A</small>3, a gun-toting helicopter drone, “autonomously performs...complex combat missions” including “targeted precision strikes”. The International Committee of the Red Cross (<small>ICRC</small>) says that many of today’s remote-controlled weapons could be turned into autonomous ones with little more than a software upgrade or a change of doctrine.</p><h2>The limits of autonomy</h2><p class="article__body-text">On May 12th the <small>ICRC</small> published a new and nuanced position on the matter, recommending new rules to regulate autonomous weapons, including a prohibition on those that are “unpredictable”, and also a blanket ban on any such weapon that has human beings as its targets. These things will be debated in December at the five-yearly review conference of the <small>UN</small> Convention on Certain Conventional Weapons, originally established in 1980 to ban landmines and other “inhumane” arms. Government experts will meet thrice over the summer and autumn, under <small>UN</small> auspices, to lay the groundwork. “There is momentum now towards a decision,” says Neil Davison, a scientific and policy adviser at the <small>ICRC</small>. Even among states who oppose new rules, he says, there is greater discussion of the limits that might need to be placed on autonomous weapons.</p><div class="advert incontent hidden advert--inline" id=""><div><div id="econ-2"></div></div></div><p class="article__body-text">Yet powerful states remain wary of ceding an advantage to rivals. In March a National Security Commission on Artificial Intelligence established by America’s Congress predicted that autonomous weapons would eventually be “capable of levels of performance, speed and discrimination that exceed human capabilities”. A worldwide prohibition on their development and use would be “neither feasible nor currently in the interests of the United States,” it concluded—in part, it argued, because Russia and China would probably cheat. <span data-ornament="ufinish">■</span></p><p class="article__body-text"><em>A version of this article was published online on May 26th, 2021</em></p><p class="article__footnote" data-test-id="Footnote">This article appeared in the Science &amp; technology section of the print edition under the headline "Fail deadly"</p><div class="layout-article-links layout-article-promo"><a class="ds-actioned-link ds-actioned-link--reuse-this-content" href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=The%20fog%20of%20war%20may%20confound%20weapons%20that%20think%20for%20themselves&amp;publicationDate=2021-05-26&amp;contentID=%2Fcontent%2F7dkp68rijbqnflj5h8fc529c3hndq6sp&amp;type=A&amp;orderBeanReset=TRUE" target="_blank"><span>Reuse this content</span></a><a class="ds-actioned-link ds-actioned-link--the-trust-project" href="https://www.economist.com/about-the-economist"><span>The Trust Project</span></a></div></div></body></html>