<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-2VYEP6CXDE"></script><script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag("js", new Date());  gtag("config", "G-2VYEP6CXDE");</script><link rel="stylesheet" href="../init.css"></head><body><section class="css-ps9sjo e1q9n20l0"><div class="css-nxgt9g e1wpftot0"><span class="css-1pipetz e1mwh2nm0"><a data-analytics="sidebar:section" href="/science-and-technology/">Science &amp; technology</a></span><span class="css-1wi1zzr e1kwye4o0"> | <!-- -->Neuroscience</span></div><h1 class="css-1bo5zl0 eoacr0f0">How neurons really work is being elucidated</h1><h2 class="css-4vhs4z ecgqxun0">That will help both medicine and the search for better artificial intelligence</h2></section><img id="myImg" src="../image/20220702_STP002.jpg" width="auto" height="200"><div class="css-8oxbol e15vdjh41"><p class="article__body-text article__body-text--dropcap" data-caps="initial"><span data-caps="initial">A</span> <small>neuron is</small> a thing of beauty. Ever since Santiago Ramón y Cajal stained them with silver nitrate to make them visible under the microscopes of the 1880s (see drawing above), their ramifications have fired the scientific imagination. Ramón y Cajal called them the butterflies of the soul.</p><style data-emotion="css drxevm">.css-drxevm{margin:1.3125rem 0 2.1875rem;}.css-drxevm figure{margin:0;min-width:100%;}.css-drxevm figure figcaption{display:inline;font-size:var(--ds-type-scale-0);}.css-drxevm audio{display:block;margin-top:0.75rem;width:100%;}.css-drxevm +figure,.css-drxevm +.g-fallback+figure{margin-top:0;}</style><div class="css-drxevm e1xre6611"><figure><div><figcaption>Listen to this story.</figcaption> <style data-emotion="css 1fs0t47">.css-1fs0t47{display:inline;font-family:var(--ds-type-system-sans);font-size:var(--ds-type-scale-0);font-style:normal;font-weight:400;line-height:var(--ds-type-leading-border-link);}.css-1fs0t47 a{color:var(--ds-color-london-5);-webkit-text-decoration:underline;text-decoration:underline;text-underline-offset:0.125rem;-webkit-transition:color var(--ds-interactions-transition),color var(--ds-interactions-transition);transition:color var(--ds-interactions-transition),color var(--ds-interactions-transition);}.css-1fs0t47 a:hover{color:var(--ds-color-chicago-30);}.css-1fs0t47 a:focus{text-decoration-color:transparent;box-shadow:0 0 0 0.125rem var(--ds-color-hong-kong-55);outline:solid transparent;}.css-1fs0t47 a:active{text-decoration-color:var(--ds-color-hong-kong-55);box-shadow:none;color:var(--ds-color-london-5);}</style><span class="css-1fs0t47 e1ncjrms0">Enjoy more audio and podcasts on<!-- --> <a href="https://economist-app.onelink.me/d2eC/bed1b25" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://economist-app.onelink.me/d2eC/7f3c199" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</span></div><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/076%20Science%20and%20technology%20-%20Neuroscience-adf88e62b1a70ea702aaa465423f166a.mp3" title="How neurons really work is being elucidated"><p>Your browser does not support the &lt;audio&gt; element.</p></audio><style data-emotion="css 1tbn2ub">.css-1tbn2ub{text-align:center;height:0;}.css-1tbn2ub>div{top:-0.625rem;}</style><div class="css-1tbn2ub e1xre6610"><style data-emotion="css sl0bi6">.css-sl0bi6{position:relative;display:inline-block;text-align:left;}</style><div class="css-sl0bi6 ey1yhxa8"><style data-emotion="css 11m9t22">.css-11m9t22{z-index:98;height:40px;width:40px;position:absolute;border-radius:50%;box-shadow:0 0 0 0 rgba(0, 0, 0, 1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1);-webkit-animation:pulse-color 2s infinite;animation:pulse-color 2s infinite;}.css-11m9t22 circle{stroke:var(--ds-color-new-york-55);opacity:0.6;}@media (max-width: 22.4375rem){.css-11m9t22{top:calc(100% - 20px);left:calc(50% - 20px);}}@media (min-width: 22.5rem){.css-11m9t22{top:calc(100% - 20px);left:calc(50% - 20px);}}@-webkit-keyframes pulse-color{0%{-webkit-transform:scale(0.95);-moz-transform:scale(0.95);-ms-transform:scale(0.95);transform:scale(0.95);box-shadow:0 0 0 0 rgba(249, 195, 31, 0.7);}70%{-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1);box-shadow:0 0 0 10px rgba(249, 195, 31, 0);}100%{-webkit-transform:scale(0.95);-moz-transform:scale(0.95);-ms-transform:scale(0.95);transform:scale(0.95);box-shadow:0 0 0 0 rgba(249, 195, 31, 0);}}@keyframes pulse-color{0%{-webkit-transform:scale(0.95);-moz-transform:scale(0.95);-ms-transform:scale(0.95);transform:scale(0.95);box-shadow:0 0 0 0 rgba(249, 195, 31, 0.7);}70%{-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1);box-shadow:0 0 0 10px rgba(249, 195, 31, 0);}100%{-webkit-transform:scale(0.95);-moz-transform:scale(0.95);-ms-transform:scale(0.95);transform:scale(0.95);box-shadow:0 0 0 0 rgba(249, 195, 31, 0);}}</style><div class="css-11m9t22 ey1yhxa6"><svg aria-hidden="true" viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg"><circle cx="50" cy="50" fill="transparent" r="18" stroke-width="12"></circle><circle cx="50" cy="50" fill="transparent" r="38" stroke-width="12"></circle></svg></div><style data-emotion="css 1uzxrld">.css-1uzxrld{z-index:99;position:absolute;}@media (max-width: 22.4375rem){.css-1uzxrld{top:100%;left:calc(50% - 130px);}}@media (min-width: 22.5rem){.css-1uzxrld{top:100%;left:calc(50% - 130px);}}</style><div class="css-1uzxrld ey1yhxa7"><style data-emotion="css 12d2uoj">.css-12d2uoj{box-sizing:border-box;background-color:white;font-family:var(--ds-type-system-sans);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:0.75rem;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding:1.5rem;-webkit-filter:drop-shadow(rgba(0, 0, 0, 0.15) 0 4px 16px);filter:drop-shadow(rgba(0, 0, 0, 0.15) 0 4px 16px);border-color:var(--ds-color-london-85);border:1px solid var(--ds-color-los-angeles-85);border-radius:0.5rem;width:260px;}</style><div class="css-12d2uoj ey1yhxa5"><style data-emotion="css 4c6dm7">.css-4c6dm7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><div class="css-4c6dm7 ey1yhxa4"><style data-emotion="css t9wkhq">.css-t9wkhq{color:var(--ds-color-london-5);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:end;-webkit-box-align:end;-ms-flex-align:end;align-items:end;font-size:var(--ds-type-scale-2);font-weight:500;line-height:var(--ds-type-leading-lower);}</style><div class="css-t9wkhq ey1yhxa3">Listen to this story</div><style data-emotion="css 1d5zrco">.css-1d5zrco{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;color:var(--ds-color-london-20);}.css-1d5zrco>button{height:24px;width:24px;}.css-1d5zrco>button .path-background{fill:var(--ds-color-london-35);}</style><div class="css-1d5zrco ey1yhxa2"><button aria-label="Close" class="ds-control-close _close" type="button"><svg viewbox="5 5 14 14" width="100%" xmlns="http://www.w3.org/2000/svg"><path class="path-background" d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg></button></div></div><style data-emotion="css 1mts01m">.css-1mts01m{color:var(--ds-color-london-35);font-size:var(--ds-type-scale--1);line-height:var(--ds-type-leading-lower);margin-bottom:1rem;font-weight:400;}</style><div class="css-1mts01m ey1yhxa1"><span>Save time by listening to our audio articles as you multitask</span></div><style data-emotion="css 1cosuf1">.css-1cosuf1{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}.css-1cosuf1>.ds-navigation-link>span{font-size:var(--ds-type-scale-0);line-height:var(--ds-type-leading-upper);color:var(--ds-color-london-35);border-bottom:1px solid var(--ds-color-london-35);}</style><div class="css-1cosuf1 ey1yhxa0"><button class="ds-slim-button" type="button">OK</button></div></div></div></div></div></figure></div><div class="article__body-text-image"><figure><div data-slim="1" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject"><meta content="https://www.economist.com/img/b/608/1342/90/media-assets/image/20220702_STC001.png" itemprop="url"/><img alt="" loading="lazy" sizes="300px" src="../image/20220702_STC001.png" srcset="../image/20220702_STC001.png"/></div></figure><p class="article__body-text">Those ramifications—dendrites by the dozen to collect incoming signals, called action potentials, from other neurons, and a single axon to pass on the summed wisdom of those signals in the form of another action potential, turn neurons into parts of far bigger structures known as neural networks. Engineers now use simulacra of these to create what they are pleased to call artificial intelligence, though it is a pale shade of the real thing.</p><p class="article__body-text">How neurons actually work their magic is only now being disentangled. One conclusion is that each is, in its own right, as powerful an information processor as a fair-sized artificial neural network. That has implications not only for learning how brains work—and how they go wrong—but also for designing artificial versions that more closely resemble the natural sort.</p><p class="article__body-text">The first widely adopted neuron model, proposed in its existing form in 1957 by Frank Rosenblatt, an American psychologist (who drew, in turn, on Alan Turing, a British computing pioneer), was the perceptron. This is a mathematical function that receives sets of binary digits (zeros and ones) as inputs. It multiplies these by numerical “weights” and then adds the products together. If the result exceeds a preordained value, the perceptron spits out a “one”. If not, it spits out a “zero”. </p></div><h2>Layer cake</h2><p class="article__body-text">To make artificial neural networks, perceptrons are encoded as software. They are organised, logically speaking, into interconnected layers and the result is trained to solve problems via feedback and feedforward loops between the layers. These loops alter the values of the weights, and thus the behaviour of the network. The more layers, the “deeper” the network. Deep neural networks now underpin everything from Google Translate to Apple’s Siri.</p><p class="article__body-text">All this imitates how action potentials arriving at the synaptic junctions between axons and dendrites, via which neurons communicate, were thought to trigger signals that then combined with each other to trigger (or not) new action potentials in the receiving cell’s axon. It is thus tempting to see neurons as physical perceptrons, with the difference from the computer versions that their signals are carried by sodium, potassium and calcium ions crossing cell membranes, rather than by a flow of electrons. And for decades that was just how many neuroscientists did see them.</p><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-1"></div></div></div><p class="article__body-text">In the early 2000s, though, Panayiota Poirazi of the Institute of Molecular Biology and Biotechnology in Heraklion, Greece, began looking at the matter differently. She imagined neurons themselves as perceptron networks. In 2003 she argued that a simple two-layer network might be enough to model them. Recent work has upped the ante. In 2021 David Beniaguev of the Hebrew University of Jerusalem concluded that, for human cortical neurons at least, five (and sometimes as many as eight) layers are needed, each with up to 256 perceptrons. </p><p class="article__body-text">This means lots of computing must be going on inside individual neurons. And it is. Dendrites are now known to generate their own, tiny action potentials, called dendritic spikes. These come in several varieties: calcium spikes (long and slow); sodium spikes (short and fast); and <small>nmda</small> spikes (triggered by a chemical called <small>n</small>-methyl-<small>d</small>-aspartate). Together, they let dendrites perform 15 of the 16 basic operations of Boolean algebra, a branch of mathematics that is the basis of digital computing. Those operations compare two input values and spit out a third as a result. Some, such as <small>and</small>, <small>or</small>, <small>not</small> and <small>nor</small>, are self-explanatory. Others, such as <small>nand</small>, <small>xnor</small> and <small>xor</small>, less so. </p><p class="article__body-text"><small>xor</small>, in particular, is notorious. It gives a non-zero output only when its inputs are dissimilar. In 1969 two eminent computer scientists, Marvin Minsky and Seymour Papert, proved that <small>xor</small> cannot be performed by a single perceptron—one of only two Boolean operation for which that is the case. This result stalled artificial-intelligence research for a decade, the first “<small>ai </small>Winter”, as it is retrospectively known. </p><p class="article__body-text">That was thought true of dendrites, too. But in 2020 work by Albert Gidon of Humboldt University, in Berlin, in which Dr Poirazi was also involved, found a new class of calcium-based spike which permits <small>xor</small>. That a single dendrite can thus outperform a perceptron suggests an entire layer of complex computation is going on out of sight of conventional models of neurons. That might help explain the remarkable performance of brains and the failure of artificial intelligence to reproduce it. </p><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-2"></div></div></div><p class="article__body-text">Axons, too, have been reassessed. The action potentials they carry had once been seen by many as analogous to the all-or-nothingness of a binary digit. Look closely, though, and action potentials vary in both height and width. That matters.</p><p class="article__body-text">In 2016 a group from the Max Planck Institute for Neuroscience, in Florida (one of the organisation’s few campuses outside its German homeland), showed that neurons in the central nervous system actively adjust the breadth of their action potentials. The following year a team from Dartmouth College in New Hampshire discovered that those in the cortex actively adjust their heights as well. </p><p class="article__body-text">Even the lengths of the intervals between action potentials matter. In May 2021 Salman Qasim of Columbia University reported that neurons in the hippocampus, a part of the brain involved in memory formation, modulate the timing of their firing to encode information about the body’s navigation through space. And in August of that year Leila Reddy and Matthew Self of the University of Toulouse, in France, reported that neurons also do this to encode the order of events in memories. </p><p class="article__body-text">All this has clinical implications. In particular, there is growing evidence that atypical dendrite development in childhood and early adulthood is linked to autism, schizophrenia and epilepsy. Deteriorating axonal function, meanwhile, is similarly associated with psychosis in multiple sclerosis, schizophrenia and bipolar disorder. These discoveries inform the development of new medicines. For example, ketamine, which triggers long-lasting structural change in dendrites, is receiving attention as a treatment for depression.</p><h2>The art of forgetting</h2><p class="article__body-text">The sophistication of the neuron and its constituent parts has also caught the attention of computer scientists. In the early 2010s deep neural networks drove such dramatic improvements in the abilities of artificial intelligence that there was genuine concern people would soon have to wrestle with machines cleverer than they were. Then, suddenly, progress stalled.</p><p class="article__body-text">Deep neural networks have hit three obstacles. First, computer scientists found that once a network has learnt a task, it struggles to transfer those skills to a new one, however similar, without extensive retraining. Second, when such a network is retrained, it tends to forget how to perform the original task—an effect called catastrophic forgetting. Third, to train a large network requires immense volumes of data, access to supercomputers, and the megawatts of electricity needed to run those supercomputers for days (or even weeks) at a time. </p><p class="article__body-text">The brain struggles with none of this. It effortlessly transfers knowledge between domains, has no trouble integrating old and new skills, and is remarkably efficient—running on watts, not megawatts. The sophistication of neurons may make the difference. In studies published last year and this, a team from Numenta, a Californian research company, designed artificial neurons, with dendrite-like subcomponents, that are immune to catastrophic forgetting. A network of these trained on 100 tasks in sequence retained the ability to perform all with reasonable accuracy. The same network also outperform networks of perceptrons at learning many tasks simultaneously.</p><div class="advert incontent hidden advert--inline"><div><div class="adcontainer" id="econ-3"></div></div></div><p class="article__body-text">Several studies show that sophisticated artificial neurons can approximate complicated functions—<small>xor</small>, for example—with greater accuracy and less energy than perceptrons do. Connected into networks, such devices learn faster and at a lower computing cost than perceptrons. The question of how brains apply knowledge from one domain to others remains a mystery, but it would not be a surprise if the complexity of neurons explains that, too. </p><p class="article__body-text">The lesson, then, is familiar: nature got there first. Necessity may be the mother of invention, but natural selection is the mother of inventors. In both neuroscience and artificial intelligence the next decade promises to be wild. Over a century after he described them, Ramón y Cajal’s butterflies are taking flight. <span data-ornament="ufinish">■</span></p><p class="article__body-text"><i>Curious about the world? To enjoy our mind-expanding science coverage, <a href="./page-not-found.html">sign up to Simply Science,</a> our weekly newsletter.</i></p></div></body></html>