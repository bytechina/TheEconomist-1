<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><link rel="stylesheet" href="../init.css"><title>Graphic Details</title></head><body><div class="ds-layout-grid ds-layout-grid--edged layout-article-header"><header class="article__header"><h1><span class="article__subheadline" data-test-id="Article Subheadline">Bias in, bias out</span><br/><span class="article__headline" data-test-id="Article Headline" itemprop="headline">Demographic skews in training data create algorithmic errors</span></h1><h2 class="article__description" data-test-id="Article Description" itemprop="description">Women and people of colour are underrepresented and depicted with stereotypes</h2></header><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://www.economist.com/graphic-detail/","name":"Graphic detail"}},{"@type":"ListItem","position":2,"item":{"@id":"https://www.economist.com/printedition/2021-06-05","name":"Jun 5th 2021 edition"}}]}
</script><div class="article__section"><strong itemprop="articleSection"><span class="article__section-headline"><a href="https://www.economist.com/graphic-detail/">Graphic detail</a></span></strong><a class="article__section-edition" data-test-id="Article Datetime" href="/printedition/2021-06-05"><span>Jun 5th 2021<!-- --> edition</span></a></div><hr class="ds-rule layout-article-header__rule"/></div><div class="ds-layout-grid ds-layout-grid--edged layout-article-body" itemprop="text"><figure class="g-fallback" data-infographic-class="g-fallback"></figure><figure><iframe data-size="full-width" height="NaN" src="20210605_GDC100_index.html"></iframe></figure><p class="article__body-text article__body-text--dropcap" data-caps="initial"><span data-caps="initial">A</span><small>LGORITHMIC BIAS</small> is often described as a thorny technical problem. Machine-learning models can respond to almost any pattern—including ones that reflect discrimination. Their designers can explicitly prevent such tools from consuming certain types of information, such as race or sex. Nonetheless, the use of related variables, like someone’s address, can still cause models to perpetuate disadvantage.</p><style data-emotion="css bvf2nr">.css-bvf2nr{padding-bottom:1.5rem;margin:1.3125rem 0 2.1875rem;padding:0;}</style><div class="layout-article-promo inline-newsletter-promo css-bvf2nr eq1ndt50"><style data-emotion="css 10eieub">.css-10eieub{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background:var(--ds-color-los-angeles-95);border:0.0625rem solid var(--ds-color-los-angeles-85);border-radius:0.125rem;color:var(--ds-color-london-5);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;font-family:var(--ds-type-system-sans);line-height:var(--ds-type-leading-lower);padding:0 var(--ds-grid-gutter) 1.5rem var(--ds-grid-gutter);text-align:center;}.css-10eieub ._content-promo-subheadline,.css-10eieub ._content-promo-tagline,.css-10eieub ._content-promo-tagline-separator{font-size:var(--ds-type-scale--3);font-weight:500;letter-spacing:0.03rem;margin-bottom:0.125rem;text-transform:uppercase;}.css-10eieub ._content-promo-tagline{color:var(--ds-color-economist-red);}.css-10eieub ._content-promo-headline{font-family:var(--ds-type-system-serif);font-size:var(--ds-type-scale-4);font-weight:700;line-height:var(--ds-type-leading-upper);}.css-10eieub ._content-promo-description{font-size:var(--ds-type-scale-0);font-weight:400;}.css-10eieub *+._content-promo-description{margin-top:0.25rem;}.css-10eieub ._content-promo-ident{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;}.css-10eieub *+._content-promo-cta{margin-top:1.5rem;}.css-10eieub ._ds-ident{display:block;padding:1rem;}@media (min-width: 37.5rem){.css-10eieub{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;padding:0.5rem 1.5rem 1.25rem 0.375rem;text-align:initial;}.css-10eieub ._content-promo-ident{-webkit-align-self:flex-start;-ms-flex-item-align:flex-start;align-self:flex-start;}.css-10eieub ._content-promo-headline{margin-top:1rem;}.css-10eieub ._content-promo-subheadline-container{margin-top:1rem;}.css-10eieub *+._content-promo-headline{margin-top:0;}.css-10eieub ._content-promo-content{margin:0 0 0 1rem;}}</style><div class="_newsletterContentPromo css-10eieub e1pgax3g0"><div class="_content-promo-ident"><span class="ds-ident-newsletters ds-ident-newsletters--the-economist-today _ds-ident"><svg class="ds-ident-newsletters__svg" height="120" viewbox="0 0 112 112" width="120" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><circle cx="55.95" cy="56" id="prefix__a" r="55.944"></circle></defs><g fill="none" fill-rule="evenodd"><mask fill="#fff" id="prefix__b"><use xlink:href="#prefix__a"></use></mask><use fill="#2E45B8" xlink:href="#prefix__a"></use><path d="M55.907 23.329c18.322 0 33.175 15.003 33.175 33.51 0 18.508-14.853 33.51-33.175 33.51-18.322 0-33.175-15.002-33.175-33.51 0-18.507 14.853-33.51 33.175-33.51z" fill="#2F44B8" mask="url(#prefix__b)"></path><path d="M35.757 83.077c-7.92-6.094-13.025-15.684-13.025-26.47 0-4.191.768-8.213 2.175-11.908L55.85 56.822 35.757 82.976" fill="#CED3F3" mask="url(#prefix__b)"></path><path d="M76.103 83.46c-5.588 4.322-12.716 6.89-20.293 6.89-7.597 0-14.832-2.547-20.427-6.89l20.448-26.565L76.103 83.46z" fill="#ADB6EB" mask="url(#prefix__b)"></path><path d="M55.851 56.895V23.217c14.296 0 26.563 9.42 31.193 22.336L55.85 56.895z" fill="#6D7BDA" mask="url(#prefix__b)"></path><path d="M87.246 45.37L55.851 56.949 76.02 83.413c7.97-6.085 13.175-15.672 13.175-26.465 0-4.008-.648-7.85-1.948-11.409v-.168z" fill="#8D98E2" mask="url(#prefix__b)"></path><path d="M55.851 57.455l-.003-34.014c-14.167 0-26.218 9.02-30.99 21.615 7.086 2.762 17.417 6.895 30.993 12.399z" fill="#EBEDFA" mask="url(#prefix__b)"></path><path d="M46.005 21.538h19.692v-6.377H46.005z" fill="#FFF" mask="url(#prefix__b)"></path><path d="M57.459 45.968l6.635.048.152 20.962-6.636-.048z" fill="#0D0D0D" mask="url(#prefix__b)" transform="rotate(45 60.852 56.473)"></path><path d="M47.553 46.016l6.635-.048-.151 20.962-6.636.048z" fill="#0D0D0D" mask="url(#prefix__b)" transform="rotate(135 50.795 56.473)"></path><path d="M55.963 87.582c-16.756 0-30.308-13.696-30.308-30.631 0-16.935 13.552-30.632 30.308-30.632s30.308 13.697 30.308 30.632c0 16.935-13.552 30.631-30.308 30.631m34.786-56.526l-4.591-4.596-4.248 4.343a36.857 36.857 0 00-5.835-4.835c-5.801-3.862-12.7-6.108-20.112-6.108-20.266 0-36.7 16.609-36.7 37.091s16.393 37.091 36.7 37.091c20.307 0 36.7-16.609 36.7-37.091 0-7.85-2.444-15.129-6.576-21.13l4.662-4.765z" fill="#FFF" mask="url(#prefix__b)"></path></g></svg></span></div><div class="_content-promo-content"><div class="_content-promo-subheadline-container"><span class="_content-promo-subheadline">The Economist Today</span></div><h2 class="_content-promo-headline">Hand-picked stories, in your inbox</h2><div class="_content-promo-description"><p>A daily email with the best of our journalism</p></div><div class="_content-promo-cta"><style data-emotion="css 1ab4byb">.css-1ab4byb .ds-form-label{display:none;}.css-1ab4byb .ds-form-notice{margin-top:1rem;}@media (min-width: 37.5rem){.css-1ab4byb .ds-form-field-combined-input{max-width:28rem;}}</style><div class="css-1ab4byb e12hvj4w0"><div class=""><label class="ds-form-label" for=""><span></span></label><div class="ds-form-field-combined-input"><input class="ds-form-input" id="" maxlength="320" name="" placeholder="example@email.com" type="email" value=""/><button class="ds-button">Sign up</button></div></div></div></div></div></div></div><p class="article__body-text">Ironing out all traces of bias is a daunting task. Yet despite the growing attention paid to this problem, some of the lowest-hanging fruit remains unpicked.</p><p class="article__body-text">Every good model relies on training data that reflect what it seeks to predict. This can sometimes be a full population, such as everyone convicted of a given crime. But modellers often have to settle for non-random samples. For uses like facial recognition, models need enough cases from each demographic group to learn how to identify members accurately. And when making forecasts, like trying to predict successful hires from recorded job interviews, the proportions of each group in training data should resemble those in the population.</p><div class="advert incontent hidden advert--inline" id=""><div><div id="econ-1"></div></div></div><p class="article__body-text">Many businesses compile private training data. However, the two largest public image archives, Google Open Images and ImageNet—which together have 725,000 pictures labelled by sex, and 27,000 that also record skin colour—are far from representative. In these collections, drawn from search engines and image-hosting sites, just 30-40% of photos are of women. Only 5% of skin colours are listed as “dark”.</p><p class="article__body-text">Sex and race also sharply affect how people are depicted. Men are unusually likely to appear as skilled workers, whereas images of women disproportionately contain swimwear or undergarments. Machine-learning models regurgitate such patterns. One study trained an image-generation algorithm on ImageNet, and found that it completed pictures of young women’s faces with low-cut tops or bikinis.</p><figure class="g-fallback" data-infographic-class="g-fallback"></figure><figure><iframe data-size="full-width" height="NaN" src="20210605_GDC200_index.html"></iframe></figure><p class="article__body-text">Similarly, images with light skin often displayed professionals, such as cardiologists. Those with dark skin had higher shares of rappers, lower-class jobs like “washerwoman” and even generic “strangers”. Thanks to the Obamas, “president” and “first lady” were also overrepresented.</p><p class="article__body-text">ImageNet is developing a tool to rebalance the demography of its photos. And private firms may use less biased archives. However, commercial products do show signs of skewed data. One study of three programs that identify sex in photos found far more errors for dark-skinned women than for light-skinned men.</p><p class="article__body-text">Making image or video data more representative would not fix imbalances that reflect real-world gaps, such as the high number of dark-skinned basketball players. But for people trying to clear passport control, avoid police stops based on security cameras or break into industries run by white men, correcting exaggerated demographic disparities would surely help.<span data-ornament="ufinish">■</span></p><p class="article__body-text" data-interactive-class="g-interactive-source">Sources: ImageNet; Google Open Images; IPUMS</p><figure data-infographic-js="1"></figure><p class="article__footnote" data-test-id="Footnote">This article appeared in the Graphic detail section of the print edition under the headline "Bias in, bias out"</p><div class="layout-article-links layout-article-promo"><a class="ds-actioned-link ds-actioned-link--reuse-this-content" href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Demographic%20skews%20in%20training%20data%20create%20algorithmic%20errors&amp;publicationDate=2021-06-05&amp;contentID=%2Fcontent%2Fmvcco4da1o83k025ti1nre2jq64s4t0j&amp;type=A&amp;orderBeanReset=TRUE" target="_blank"><span>Reuse this content</span></a><a class="ds-actioned-link ds-actioned-link--the-trust-project" href="https://www.economist.com/about-the-economist"><span>The Trust Project</span></a></div></div></body><script>
    window.tedl = window.tedl || {};
    // Resize iframes on articles with interactives when they send a RESIZE message
    window.addEventListener('message', (event) => {
    if (event.data.type === 'RESIZE') {
    const height = parseInt(event.data.payload.height, 10);
    Array.prototype.forEach.call(document.getElementsByTagName('iframe'), function (element) {
    if (element.contentWindow === event.source) {
    element.style.height = height + 'px';
    }
    });
    }
    }, false);
    </script></html>